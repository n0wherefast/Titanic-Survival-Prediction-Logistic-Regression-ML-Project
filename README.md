# ğŸ›³ï¸ Titanic Survival Prediction â€“ Logistic Regression ML Project

![Python](https://img.shields.io/badge/Python-3.10+-blue?logo=python)
![Scikit-Learn](https://img.shields.io/badge/Scikit--Learn-Modeling-orange?logo=scikit-learn)
![SHAP](https://img.shields.io/badge/Explainability-SHAP-red)
![Status](https://img.shields.io/badge/Status-Completed-brightgreen)

> **Goal:** Predict whether a passenger survived the Titanic disaster using a logistic regression model, feature engineering, and explainable AI (SHAP).

---

## ğŸ“‹ Table of Contents
- [ğŸ“– Introduction](#-introduction)
- [ğŸ“‚ Project Structure](#-project-structure)
- [ğŸ§¹ Data Preprocessing](#-data-preprocessing)
- [ğŸ§© Feature Engineering](#-feature-engineering)
- [âš™ï¸ Model Training](#ï¸-model-training)
- [ğŸ“ˆ Model Evaluation](#-model-evaluation)
- [ğŸ§  Explainability (SHAP)](#-explainability-shap)
- [ğŸ® Interactive Prediction](#-interactive-prediction)
- [ğŸš€ Quick Start](#-quick-start)
- [ğŸ› ï¸ Tools & Requirements](#ï¸-tools--requirements)
- [ğŸ—ï¸ Future Improvements](#ï¸-future-improvements)
- [ğŸ‘¤ Author](#-author)

---

## ğŸ“– Introduction

This notebook-based project explores the famous **Kaggle Titanic dataset** to predict passenger survival using **Logistic Regression**.

It covers the full **data science pipeline**:
1. Data loading & cleaning  
2. Feature engineering  
3. Model training with class balancing  
4. Evaluation (accuracy, recall, F1, confusion matrix)  
5. Explainability using **SHAP**  
6. Interactive prediction for new passengers  

This project is ideal as a **portfolio example** of applied data science and model interpretability.

---

## ğŸ“‚ Project Structure

titanic-logistic-regression/
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ U4_04_train.csv
â”‚
â”œâ”€â”€ notebook/
â”‚ â””â”€â”€ Titanic_Logistic_Regression.ipynb
â”‚
â”œâ”€â”€ models/
â”‚ â””â”€â”€ titanic_model.pkl
â”‚
â”œâ”€â”€ images/
â”‚ â”œâ”€â”€ confusion_matrix.png
â”‚ â”œâ”€â”€ shap_summary.png
â”‚ â””â”€â”€ shap_waterfall.png
â”‚
â”œâ”€â”€ export_notebook.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


---

## ğŸ§¹ Data Preprocessing

Key preprocessing steps:
- Missing values handled via **KNNImputer** (`Age`)
- Dropped non-informative columns: `Cabin`, `Name`, `Ticket`, `PassengerId`
- One-hot encoding of categorical variables (`Sex`, `Embarked`)
- Removal of remaining missing rows after cleaning

Example:
```python
from sklearn.impute import KNNImputer

imputer = KNNImputer(n_neighbors=5)
df[['Age']] = imputer.fit_transform(df[['Age']])


